AWS STORAGE -part1 

types of storage 

simple storage service s3

Elastic file system Efs

Elastic block storage (EBS)

Glacier

Snowball

AWS offers a complete reange of cloud storage services to support both appplication and archival complicance requirements select from objects file and black storage services
as well as cloud data migration options to start desigining the foundation of your cloud IT envionment 

							******PART-02******

Block v/s object storage 

	* Block storage is sutiable for transational databases, random read/write loads and structured database storage

	* Block storage divides the data to be stored in evenly sized block (data chunk) for instance a file can be split into evenly sized blocks before it is stored

	* Data blocks stored in block storage would not contain metodata (data reated, data modified, content type etc.

	* Block sotrage only keeps the address (into) where the data blocks one stored, it does not core what is in that block, just how to retrieve it when required 

	* Object storage stores the files as a whole and does not divide them

	* In object storage on object is 

		-The file/data it self	
		-its metadata
		-object global unique id 

	* The object global unique Id, is a unique identifier for the object (can be the object name itself and it must be unique such that it can be retiseved disregarding
	  where its physical storage location is 

	* Object storage cannot be mounted as a drive 
		Example of object storage solutions- Dropbox, AWS S3 facebook 

							*******PART-03*****

Simple Storage service S3:-

	*S3 is a storage for the internet it has a simple web serivces interface for simple storing & reteriving of any amount of ddata anytime forom anywhere on the internet

	*S3 is object based storage 

	*You cannot install operating system on s# 

	*S3 has a distributed data-store architecture where objects are redindantly stored in multiple location (min 3 location in same region)

	*Data is stored in bucket 

	* a bucket is a flat container of objects 

	*Max capacity of a bucket is 5TB

	*You can create folders in your bucket (available through console)

	*you cannot create nested buckets 

	*Bucket ownership is non-transferrable 

	*S3 Bucket is region specific

	*You can have upto 100 bukets per account (max expand on request)

S3 bukets-noming rules

	*S3 buckets names (keys) are globallly unique accross all AWS regions

	*Buckets names cannnot be change after they are created 

	*If a bucket is deleted, its name becomes available again to you or other account to uise 

	*Bucket names must be atleast 3 and no more the 63 characters long 

	*Bucket names are part of the URL used to access a bucket 

	*Bucket name must be a series of one or more labels (xyz buckets)

	*Bucket names can contain lowercase numbers and hyphen cannot use uppercase letter

	*Each label must start and end with a lowercase letter or a number

	*by default. Buckets and its objects are private by default only owener can access the bucket 

S3 Buckets subresources 

	*Sub-resources for s3 buckets includes-

	Lifecycle - to decide on objects lifecycle management 

	Website - To hold configuration related to static website hosted in S3 buckets 

	Versioning - Keeep object versions as it changes (get updated)

	Access control List - Bucket policies 

	The name is simple two parts Bucketregions endpoint/bucket name 

	Example - To S3 bucket named mybucket in europe west regions 

S3 objects

	*An object size stored in an s3 Bucket can be 0 byte to 5 TB

	*Each object is stored and retrised by a unique key (id or name )

	*An object in AWS S3 is uniquely identified and addressed through 

	- service endpoint
	-Bucket name 
	- object key (name)
	- Optionally object verison 

	*Object stored in a s3 bucket in a region will never leave that region unless you specifially move then to another region or CRR

	*A bucket owner can grant Cross-account  permission to another AWS account (OR USERS IN ANOTHER ACCOUNT ) to upload object

	*You can grant s3 bucket/object permission to 

	- Individual users
	_ AWS account
	- Make the resource public 
	- Or to all authenticate users

								******PART-04*********

S3 bucket versioning 

	*Bucket versioing is a s# bucket sub-resouce used to  protect against accidental object/data deletion or overwrites
	
	*Versioing can also be used for data retention and archive 

	*Once you enable versioing on a bucket, it cannot be disabled, howeveri it can suspended

	*When enabled, bucket versioing will protect existing and new objects and maintans their versions as they are upeded

	*Updating, onjects refers to put POST, COPY, DELETE actions an objects 

	*When versioing is enabled, and you try to delete an object, a delette maker is placed on the object

	- You can still view the object and the delete marker

	*if you reconsider deleting the objects, you can delete the "Delete marker" and the object will be availabel again

	*You will be charged for all S3 storage cost for all object versions stored 

	*You can use versioing with s3 lifecycle polices to delete older versions, or you can move them to a ceaper s# storage (or Glacier)

	Bucket versioning State-

	- Enabled

	- Suspended

	- Un-versioned

	*versioning applies to all objects in a bucket and not portially applied 

	*object existing before enabling versioning will have a version  ID or NULL 

	*If you have a bucket that is already versioned, then you supend versioning existing object and their versions remains as it is 

	- Howerver they will not be updated versioned further with future uploded while the buckets versioning is suspeneded 

	- New object (Uploaded agter suspension) they will have a version Id "Null" if the same key (name) is used to store another objects, it will overide the exsting one

	*An object deletions in a suspended versioning bukcets will only delete the objects with ID "null"\

S3 Bucket versioning - MFA Delete 

	*Multifacor authentication delete is a versioning capacity that adds another level of security in case your account is compromised

	*This adds another layer of security for the folling:-

	- Changing your buckets versioning state

	- Permanetly deleting on object version 

	*MFA Delete Requires:-

	-your security credentails

	-The code displayed on an approved physical or s\w vased authentication device 

S3 multipart upload 

	*Is used to upload on object in parts

	*Parts are uploaded in deperndetly and in parallel, in any order

	*It is recommended for objects sizes of 100 MB or larger 

	*You must use it for objects larger than 5 GB

	*This is done through s# multipart upload API 

Copying s3 objects 

	*The copy operation creates a copy of an object that is already stored in Amazon S3 

	*You can create a copy of your object upto 5GB in size in a single atomic operation

	*However to copy an object greater than 5GB you must use the multipart upload API

	*Incur charges if copy to another region 

Use the copy operation to -

	*Generate additional copies of the subject

	*Renaming object (copy to a new name)

	*Changing the copy's storage class or encrypt it or rest

	*Move object across AWS location region

	*Change object metadat									


									*******PART-05********

Storage  classes of Amazon s3 

	Amazon S3-standard
	
	Amazon S3 Glacier Deep archive --- chepest 
	
	Amazon Glacier -- long term storage

	Amazon S3 standard inferquent Access ---cost less but you pay to access it more frequently -- standard -IA 

	Amazon s3 one-zone-IA 
	
	Amazon S3 inteligent testing 

Amazon S3 standard

	*S3 standart offers high durability availability and performance object storage for frequently accessed data 

	*durability is 99.99999999/

	*Designed for 99.99/ availability over a given year

	*Support SSL For data in transit and encryption of data at rest 

	*The storage cost for the object is fairly high, but ther is very less change for accessing the objects

	*Largest object that can be uploded in a single PUT in 5GB 

Amazon s3-IA

	*S3-IA is for data that is accocied less frequently but requires rapid access when needed

	*The storage cost is much changes then s3 standard almost half the price but you are charged more heavily for assessing your objects

	*durability is 99.9999999/

	*Resistent against events that impact on entire AZ 

	*Availability is 999/ in year

	*Support SSL for data in transpit and encrption of data at rest

	*Data that is deleted from S3-IA within 30 days will be charged for a full 30 days 

	*Blocked with the Amazon S3 service level agreement for availability

Amazon s3 inteligent testing 

	*The s3 inteligent tisting storage class is designed to optimize cost by automatically moving data to the most cost-effective adcess-tier

	*It works by storing objects in two access tiers

	*If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier

	*Ther are no reterval fees when using the s3-intelligent tiering storage class and no additonal tiering fee when object are moved between access tiers

	*Some low latency and high performance of s3-standard 

	*object less than 128 consist move to IA

	*Durability is  99.99999999%

	*Avalibility is 99.9%

Amazon one-zone-IA 

	*s3 one zone-IA is for data that is accessed less frequently, but requires copied access when needed

	*Data store in single AZ

	*Ideal for these who want lower cost option of IA-data 

	*It is good choice for stoining secondary backup coies of on-premises data or easily recreatable data

	*You can use s3 lifecycle policies 

	*Durability is 99.9999999%

	*availbility is 99.5%

	Because s3 one zone-IA stores data in a sintgle AZ data stored in this strage class will be lost in the event of AZ destruction 

Amazon S3 glacier

	*S3 Glacier is a secure clearable low cost storage class for data archiving

	*To keep cost low yet suitable for varying needs s3 glacier provied there retrival options that range from a few minutes to hours

	*You can upload object directly to Glicer or use lifecyle policies

	*Duration is 99.999999%

	*Data is resilent in the event of one entire AZ destination

	*Support SSL for data in transit and encryption data at rest

	*You can retriver 10GB of your amazon s3 glacier data per month for free with free tier account 

Amazon s3 glacier deep Archive 

	*S3 glacier deep archive is amazon s3 cheapest storage 

	*Design to retain data for long perriod eg- 10 years

	*All objects stored in s3 glacier deep archive are replicated and stored across atleast at three geographically-dispersed AZ 

	*Duration is 99.999999%

	*Ideal alternative to magnetic tape librories

	*Retrival time within 12 hours

	*Storage cost is upto 75% less than for the existing s3-glocier storage class

	*Availability is 99.9%